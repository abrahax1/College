{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the ID3 Algorithm in Python\n",
    "\n",
    "\n",
    "In this section of the notes we will implement in Python the ID3 algorithm and all the functions and data structures that it uses, including:\n",
    "\n",
    "- Step 1. a function to compute the **entropy** of a set\n",
    "- Step 2. a function to **partition a dataset** based on the levels of a feature.\n",
    "- Step 3. a function to compute the **remaining entropy** after a dataset has been partitioned.\n",
    "- Step 4. a function to compute the **information gain** of a feature in a dataset.\n",
    "- Step 5. a representation of a **decision tree**.\n",
    "- Step 6. the **ID3** algorithm.\n",
    "- Step 7. a function that prints out a decision tree so that we can see what the tree looks like.\n",
    "- Step 8. a function that takes a decision tree object and a query and returns a **prediction**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need is to get the dataset that we will use to train the decision tree model. To keep things simple at the start we will simply hardcode this data into three data structures:\n",
    "\n",
    "1. **feature_names** contains the names of the features in the dataset,\n",
    "2. **feature_levels** is a dictionary that lists for each feature all the levels of that feature\n",
    "3. **dataset** is a 2 dimensional data structure that stores the descriptions of the instances in the dataset.\n",
    "\n",
    "(Or you can read the data from a file the same way as previous labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the names of the features in the dataset\n",
    "feature_names = ['stream', 'slope', 'elevation', 'vegetation']\n",
    "\n",
    "# dictionary object that lists for each feature (key) the set of levels in the domain of the feature (value)\n",
    "feature_levels = {'stream':['false','true'], \n",
    "                 'slope':['flat','moderate','steep'], \n",
    "                 'elevation':['low','medium','high','highest'], \n",
    "                 'vegetation':['chaparral','riparian','conifer']}\n",
    "\n",
    "# Then we create our dataset of training instances\n",
    "# the first row in the dataset (dataset[0]) lists the names of the features\n",
    "dataset=[['false','steep','high','chaparral'],\n",
    "          ['true','moderate','low','riparian'],\n",
    "          ['true','steep','medium','riparian'],\n",
    "          ['false','steep','medium','chaparral'],\n",
    "          ['false','flat','high','conifer'],\n",
    "          ['true','steep','highest','conifer'],\n",
    "          ['true','steep','high','chaparral']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the information in these lists using standard Python notation.\n",
    "\n",
    "For example, the following commands output: \n",
    "1. the name of the target feature, \n",
    "2. the target feature values for the instances in the dataset, \n",
    "3. the description of the 3rd instance in the dataset, \n",
    "4. and the value of the 'elevation' feature for the 5th instance in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target feature: ['vegetation']\n",
      "Instance 1 Target Feature Value = ['chaparral']\n",
      "Instance 2 Target Feature Value = ['riparian']\n",
      "Instance 3 Target Feature Value = ['riparian']\n",
      "Instance 4 Target Feature Value = ['chaparral']\n",
      "Instance 5 Target Feature Value = ['conifer']\n",
      "Instance 6 Target Feature Value = ['conifer']\n",
      "Instance 7 Target Feature Value = ['chaparral']\n",
      "Instance 3 = ['true', 'steep', 'medium', 'riparian']\n",
      "The elevation feature for instance 5 in the datset is = high\n"
     ]
    }
   ],
   "source": [
    "# 1. The name of the target feature\n",
    "# The target feature name is listed in the last element of feature_names\n",
    "print(\"Target feature: \" + str(feature_names[-1:]))\n",
    "\n",
    "# 2. List the target feature values for the instances in the dataset\n",
    "for i in range(0, len(dataset)):\n",
    "    print(\"Instance \" + str(i+1) + \" Target Feature Value = \" + str(dataset[i][-1:]))\n",
    "\n",
    "# 3. The description of the 3rd instance in the dataset\n",
    "# Remember, Python lists are zero indexed.  \n",
    "print(\"Instance 3 = \" + str(dataset[2]))\n",
    "\n",
    "# 4. The value of the 'elevation' feature for the 5th instance in the dataset\n",
    "index = feature_names.index('elevation')\n",
    "print(\"The elevation feature for instance 5 in the datset is = \" + str(dataset[4][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. A function to compute the **entropy** of a set\n",
    "\n",
    "\n",
    "The basic building block for information based learning is the definition of a function that can return the **entropy** of a set.\n",
    "\n",
    "Recall that entropy is essentially the measure of disorder (or the heterogeneity) of a set. **(The formula for entropy is given in lectures)**\n",
    "\n",
    "Write a function that takes a list of values as input and return the entropy of the list\n",
    "\n",
    "`\n",
    "def entropy(values):\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a list of values as input and return the entropy of the list\n",
    "from math import log2\n",
    "from turtle import left\n",
    "\n",
    "def entropy(values):\n",
    "  # Computes entropy of label distribution\n",
    "  set1 = set(values)\n",
    "  count = []\n",
    "\n",
    "  prob = 0.0\n",
    "  for i in set1:\n",
    "    count += [values.count(i)]\n",
    "\n",
    "  # Compute entropy\n",
    "  for i in count:\n",
    "    prob += -(i / len(values) * log2(i/ len(values)))\n",
    "    \n",
    "  return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your entropy function on the lists below. \n",
    "\n",
    "`animalset1 = ['cat', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse']`\n",
    "\n",
    "`animalset2 = ['horse', 'horse', 'horse', 'horse', 'horse', 'dog', 'dog', 'dog', 'dog', 'dog']`\n",
    "\n",
    "`animalset3 = ['horse', 'horse', 'dog', 'dog', 'cat', 'cat', 'sheep', 'sheep', 'lion', 'lion']`\n",
    "\n",
    "\n",
    "You should get the following outputs:\n",
    "\n",
    "`\n",
    "Entropy animalset1 = 0.4394969869215134\n",
    "Entropy animalset2 = 1.0\n",
    "Entropy animalset3 = 2.321928094887362\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy animalset1 = 0.4394969869215134\n",
      "Entropy animalset2 = 1.0\n",
      "Entropy animalset3 = 2.321928094887362\n"
     ]
    }
   ],
   "source": [
    "animalset1 = ['cat', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse', 'horse']\n",
    "print(\"Entropy animalset1 = \" + str(entropy(animalset1)))\n",
    "\n",
    "animalset2 = ['horse', 'horse', 'horse', 'horse', 'horse', 'dog', 'dog', 'dog', 'dog', 'dog']\n",
    "print(\"Entropy animalset2 = \" + str(entropy(animalset2)))\n",
    "\n",
    "animalset3 = ['horse', 'horse', 'dog', 'dog', 'cat', 'cat', 'sheep', 'sheep', 'lion', 'lion']\n",
    "print(\"Entropy animalset3 = \" + str(entropy(animalset3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When constructing our decision tree we will  need to be able to extract all values of a feature, and specifically all values of the target feature.\n",
    "\n",
    "Next we will write a helper function \n",
    "`def get_feature_column(feature_index=-1, dataset=None):\n",
    "`\n",
    "that will extract a column of values from the dataset (two-dimentional list) and returns it as a list\n",
    "\n",
    "This function takes two arguments: \n",
    "1. the index of the feature that we want to extract the values of from the dataset. \n",
    "2. the dataset that we wish to extract the values from.\n",
    "\n",
    "Implement that function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function extracts a column of values from a table and returns it as a list\n",
    "\n",
    "def get_feature_column(feature_index=-1, dataset=None):\n",
    "    col = []\n",
    "    \n",
    "    for i in dataset:\n",
    "        counter = 0\n",
    "        for j in i:\n",
    "            if counter == 3:\n",
    "                col += [j]\n",
    "                counter += 1\n",
    "            else:\n",
    "                counter += 1\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column = ['chaparral', 'riparian', 'riparian', 'chaparral', 'conifer', 'conifer', 'chaparral']\n",
      "Entropy of dataset1: 1.5566567074628228\n"
     ]
    }
   ],
   "source": [
    "# Let's test it.\n",
    "# We assume that the target feature is the rightmost column in the dataset\n",
    "# so to get the index of this feature we simply subtract 1 from the length of the first row in the dataset. \n",
    "\n",
    "target_index = len(dataset[0])-1\n",
    "\n",
    "\n",
    "# We can then use the get_feature_column function to extract the feature column into a list\n",
    "\n",
    "target_column = get_feature_column(target_index,dataset)\n",
    "print(\"Target column = \" + str(target_column))\n",
    "print(\"Entropy of dataset1: \" + str(entropy(target_column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get the following output:\n",
    "    \n",
    "`\n",
    "Target column = ['chaparral', 'riparian', 'riparian', 'chaparral', 'conifer', 'conifer', 'chaparral']\n",
    "Entropy of dataset1: 1.5566567074628228\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2+3. Partitioning a Dataset by a Feature and Calculating the Remaining Entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ID3** algorithm builds decision trees by recursively splitting the dataset using the descriptive feature that has the **highest information gain**, i.e., the largest reduction in **entropy** in terms of the target feature after we split the dataset using the values of a descriptive feature. \n",
    "\n",
    "\n",
    "\n",
    "We have already defined the **entropy** function but we still need to define two other functions before we can compute the information gain for a feature: \n",
    "\n",
    "1. the first function we need to define is a **function that returns the partitions that are created if we split a dataset using a particular feature**. We will call this function **create_partitions**\n",
    "\n",
    "2. the second function that we need to define is a function that can **calculates the entropy that remains after we split the dataset** up into partitions. This function is called **calculate_remainder** and implements the formula for remainder from lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the function \n",
    "def create_partitions(featureIndex=-1,dataset=None):\n",
    "    partitions={}\n",
    "    for i in range(0, len(dataset)):\n",
    "        tmp_value = dataset[i][featureIndex]\n",
    "        tmp_list = []\n",
    "\n",
    "        if tmp_value in partitions.keys():\n",
    "            tmp_list = partitions[tmp_value]\n",
    "            tmp_list.append(dataset[i])\n",
    "        else:\n",
    "            tmp_list.append(dataset[i])\n",
    "\n",
    "        partitions[tmp_value]=tmp_list\n",
    "\n",
    "    return(partitions)\n",
    "\n",
    "def calculate_remainder(partitions):\n",
    "    remainder=0\n",
    "    example_instance=(next(iter(partitions.values())))[0]\n",
    "    target_index=len(example_instance)-1\n",
    "    size_dataset =0\n",
    "\n",
    "    for k in partitions.keys():\n",
    "        size_dataset = size_dataset + len(partitions[k])\n",
    "\n",
    "    for k in partitions.keys():\n",
    "        tmp_partition = partitions[k]\n",
    "        target_column = get_feature_column(target_index,tmp_partition)\n",
    "        ent = entropy(target_column)\n",
    "        weight = len(tmp_partition)/size_dataset\n",
    "        remainder = remainder + (weight * ent)\n",
    "\n",
    "    return(remainder)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your functions\n",
    "\n",
    "The following Python code illustrates how we can: \n",
    "\n",
    "1. use the create_partitions function to split a dataset using a particular feature - in this example we have arbitrarily decided to use the 'slope' feature (index = 1) \n",
    "\n",
    "2. use the calculate_remainder function to calculate the entropy remaining after we split the dataset using the 'slope' feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paritions created:\n",
      "---------------------\n",
      "steep:\n",
      "\t\t['false', 'steep', 'high', 'chaparral']\n",
      "\t\t['true', 'steep', 'medium', 'riparian']\n",
      "\t\t['false', 'steep', 'medium', 'chaparral']\n",
      "\t\t['true', 'steep', 'highest', 'conifer']\n",
      "\t\t['true', 'steep', 'high', 'chaparral']\n",
      "moderate:\n",
      "\t\t['true', 'moderate', 'low', 'riparian']\n",
      "flat:\n",
      "\t\t['false', 'flat', 'high', 'conifer']\n",
      "---------------------\n",
      "Remaining entropy after partitioning: 0.9792504246104776\n"
     ]
    }
   ],
   "source": [
    "#use the create_paritions function to split the dataset using descriptive feature 'slope' (feature index = 1)\n",
    "slope_partitions = create_partitions(1,dataset)\n",
    "print(\"Paritions created:\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "for k in slope_partitions.keys():\n",
    "    print(str(k)+ \":\")\n",
    "    instances = slope_partitions[k]\n",
    "    for inst in instances:\n",
    "        print('\\t\\t' +str(inst))\n",
    "\n",
    "print(\"---------------------\")\n",
    "\n",
    "    \n",
    "#use the calculate_remainder function to calculate the entropy remaining after we split dataset using the slop feature\n",
    "rem = calculate_remainder(slope_partitions)\n",
    "print(\"Remaining entropy after partitioning: \" + str(rem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Implementing Information Gain\n",
    "\n",
    "We are now ready to implement the function that specifies the information gain metric that the ID3 algorithm uses to select which feature to partition the dataset on.\n",
    "\n",
    "The function implements the information gain as discussed in lectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(feature_index=-1, dataset=[]):\n",
    "\n",
    "    #calculate the entropy of the dataset before we partition it using the feature \n",
    "\n",
    "    target_index = len(dataset[0])-1\n",
    "    target_column = get_feature_column(target_index,dataset)\n",
    "    \n",
    "    h = entropy(target_column)\n",
    "\n",
    "    #calculate the remaining entropy after we partition the dataset using the feature\n",
    "    partitions = create_partitions(feature_index,dataset)\n",
    "\n",
    "    rem = calculate_remainder(partitions)\n",
    "\n",
    "    #calculate the information gain for the feature\n",
    "    ig = h - rem\n",
    " \n",
    "    return(ig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your function **information_gain** to calculate the information gain for each of the descriptive features in the dataset.\n",
    "\n",
    "You should get the following outputs:\n",
    "\n",
    "`\n",
    "The information gain for feature stream is: 0.30595849286804166\n",
    "The information gain for feature slope is: 0.5774062828523452\n",
    "The information gain for feature elevation is: 0.8773870642966131\n",
    "`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The information gain for feature stream is: 0.30595849286804166\n",
      "The information gain for feature slope is: 0.5774062828523452\n",
      "The information gain for feature elevation is: 0.8773870642966131\n"
     ]
    }
   ],
   "source": [
    "# Iterate across the indexes of the descriptive features in the dataset\n",
    "# (we assume that the target feature is the last feature in the dataset)\n",
    "# and call the information_gain function for each descriptive feature by passing in the feature index\n",
    "\n",
    "for i in range(0, len(dataset[0])-1):\n",
    "\n",
    "    ig = information_gain(i,dataset)\n",
    "\n",
    "    print(\"The information gain for feature \" + feature_names[i] + \" is: \" + str(ig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. The Decision Tree Representation\n",
    "\n",
    "\n",
    "We are nearly ready to implement the ID3. Before we do this, however, we need to define the data structure that we will use to represent the decision model generated by the algorithm.\n",
    "\n",
    "A decision tree is made up of a set of nodes with branches between the nodes. Each of the internal nodes in the tree records a test of a feature in the dataset and there is a branch from each internal node for each level in the domain of the tested feature. Each of the leaf nodes in the tree stores the set of instance that ended up at that leaf node during training. Indeed, the prediction returned by a leaf node is the the majority level of the target feature in this set of instances at the node.\n",
    "\n",
    "The fact that a decision tree is composed of nodes with links between nodes means that we can define a data structure for a decision tree by defining a data structure that store the information for a node. Below we define a class that stores the information stored in a node of a decision tree. We also include a number of helper functions in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tree_node:\n",
    "  def __init__(self, feature_name='', feature_index='', branches={}, instances=[], prediction=''):\n",
    "\n",
    "    self.feature_name=feature_name    #stores the name of the feature tested at this node\n",
    "    self.feature_index=feature_index  #stores the index of the feature column in the dataset\n",
    "    self.branches=branches          #a dictionary object: each key=level of test feature, each value=child node of this node\n",
    "    self.instances=instances        #in a leaf node this list stores the set of instances that ended up at the leaf\n",
    "    self.prediction=prediction      #in a leaf node this variable stores the target level returned as a prediction by the node\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Implementing the ID3 Algorithm\n",
    "\n",
    "We are now ready to implement the ID3 algorithm. (The course textbook lists a pseudocode of the algorithm if you need to look at it in more detail - See Algorithm 4.1)\n",
    "\n",
    "To keep the implementation of the ID3 algorithm readable we define two helper functions: \n",
    "\n",
    "1. **all_same** -  this function takes a list of instances and returns true if the list is non-empty and all the instances have the same target feature level. \n",
    "2. **majority_target_level** - this function takes a dataset as a parameter and returns the majority target level in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return true if all the instances in the dataset D have the same target level\n",
    "# A handy way to check for this condition is by checking if the entropy of the \n",
    "# aataset with respect to the target feature == 0\n",
    "\n",
    "def all_same(D=[]):\n",
    "\n",
    "    if len(D) > 0:\n",
    "        target_index = len(D[0])-1\n",
    "        target_column = get_feature_column(target_index,D)\n",
    "\n",
    "        if entropy(target_column) == 0:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "    \n",
    "\n",
    "#return the majority target level in the instances list\n",
    "def majority_target_level(D):\n",
    "\n",
    "        #assume the target feature is the last feature in each instance\n",
    "        target_index = len(D[0])-1\n",
    "\n",
    "        #extract the set of target levels in the instances at this node\n",
    "        target_column = get_feature_column(target_index,D)\n",
    "\n",
    "        #create a dictionary object that records the count for each target level\n",
    "        levels_count = {}\n",
    "\n",
    "        for l in target_column:\n",
    "            if l in levels_count.keys():\n",
    "                levels_count[l]+=1\n",
    "            else:\n",
    "                levels_count[l]=1\n",
    "\n",
    "        #find the target level with the max count\n",
    "        #for ease of implementation we break ties in max counts\n",
    "        #by symply returning the first level we find with the max count\n",
    "\n",
    "        max_count = -999999\n",
    "        majority_level = ''\n",
    "\n",
    "        for k in levels_count.keys():\n",
    "            if levels_count[k] > max_count:\n",
    "                max_count=levels_count[k]\n",
    "                majority_level=k\n",
    "\n",
    "        return(majority_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This **ID3** implementation takes 5 parameters. \n",
    "# The 5 parameters are as follows:\n",
    "#1. d = list of descriptive features not yet used on the path from the root to the current node \n",
    "#2. D = the set of training instances that have descended the path to this node\n",
    "#3. parentD = the set of training instances at the parent of this node\n",
    "#4. feature_levels = a dictionary object that lists for each feature (key) the set of levels in the domain of the feature (value)\n",
    "#5. feature_names = a list of the names of the features in the dataset\n",
    "\n",
    "def id3(d=[], D=[], parentD=[], feature_levels={}, feature_names=[]):\n",
    "\n",
    "    if all_same(D):\n",
    "\n",
    "        return tree_node(feature_name='',feature_index=-1,branches={},instances=D,prediction=D[0][len(D[0])-1])\n",
    "\n",
    "    elif len(d) == 0:\n",
    "\n",
    "        return tree_node(feature_name='',feature_index=-1,branches={},instances=D,prediction=majority_target_level(D))\n",
    "\n",
    "    elif len(D) == 0:\n",
    "\n",
    "        return tree_node(feature_name='',feature_index=-1,branches={},instances=D,prediction=majority_target_level(parentD))\n",
    "\n",
    "    else:\n",
    "\n",
    "        d_best = \"\"\n",
    "        best_index = -1\n",
    "        max_IG = -9999\n",
    "\n",
    "        for f in d:\n",
    "\n",
    "            feature_index = feature_names.index(f)\n",
    "            tmp_IG = information_gain(feature_index,D)\n",
    "\n",
    "            if tmp_IG > max_IG:\n",
    "                max_IG = tmp_IG\n",
    "                d_best = f\n",
    "                best_index=feature_index\n",
    "\n",
    "        node = tree_node(feature_name=d_best,feature_index=best_index,branches={},instances=[],prediction='')\n",
    "\n",
    "        #partition the dataset using the feature with the highest information gain\n",
    "        partitions=create_partitions(best_index,D)\n",
    "\n",
    "        #remove d_best from the list of features passed down to the children of this node\n",
    "        d_new = [ f for f in d if not f.startswith(d_best) ]\n",
    "\n",
    "        #iterate across all the levels of the feature and create a branch for each level\n",
    "        #we use arg4 for this because it may be that one or more of the levels of the feature do not appears in D\n",
    "\n",
    "        for level in feature_levels[d_best]:\n",
    "\n",
    "            if level in partitions.keys():\n",
    "                d_new_1 = partitions[level]\n",
    "\n",
    "            else:\n",
    "\n",
    "                #if there is a feature level that does not occur in D\n",
    "                #then create a child node where the set of training instances\n",
    "                #at the node is empty\n",
    "\n",
    "                d_new_1 = []\n",
    "\n",
    "            node.branches[level]=id3(d_new,d_new_1,D,feature_levels,feature_names)\n",
    "\n",
    "        return(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our IT3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#featureNames[:-1] is the list of features names in the dataset apart from the last feature (i.e., excluding the target feature)\n",
    "\n",
    "tree = id3(feature_names[:-1], dataset, dataset, feature_levels, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Displaying the tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--elevation:low\n",
      "   prediction = riparian\n",
      "--elevation:medium\n",
      "----stream:false\n",
      "     prediction = chaparral\n",
      "----stream:true\n",
      "     prediction = riparian\n",
      "--elevation:high\n",
      "----slope:flat\n",
      "     prediction = conifer\n",
      "----slope:moderate\n",
      "     prediction = chaparral\n",
      "----slope:steep\n",
      "     prediction = chaparral\n",
      "--elevation:highest\n",
      "   prediction = conifer\n"
     ]
    }
   ],
   "source": [
    "#This function prints out the tree in text format\n",
    "\n",
    "def print_tree(tree, indent='-'):\n",
    "\n",
    "    if tree.prediction == '':\n",
    "\n",
    "        indent+=\"--\"\n",
    "        for level in tree.branches.keys():\n",
    "            print(indent+tree.feature_name + ':' + str(level))\n",
    "            print_tree(tree.branches[level],indent)\n",
    "    else:\n",
    "\n",
    "        s = ''\n",
    "        for c in indent:\n",
    "            s+=' '\n",
    "        print(s+\" prediction = \" + tree.prediction)\n",
    "\n",
    "        \n",
    "#Here we call the function and pass in the tree we want to output\n",
    "print_tree(tree,\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Using the Tree to Make Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function returns a prediction from a tree for a query instance\n",
    "def make_prediction(query, tree, feature_levels):\n",
    "\n",
    "    if tree.prediction != '':\n",
    "\n",
    "        #if we have reached a leaf node return the prediction\n",
    "        return tree.prediction\n",
    "\n",
    "    else:\n",
    "\n",
    "        #otherwise descend the tree.\n",
    "        #1. get the level of the query instance for the node test feature\n",
    "\n",
    "        level = query[tree.feature_index]\n",
    "        for l in feature_levels[tree.feature_name]:\n",
    "\n",
    "            if l.startswith(level):\n",
    "                #2. find the branch that matchs this level and desencd the branch\n",
    "                return make_prediction(query,tree.branches[level],feature_levels)\n",
    "\n",
    "        print(\"No prediction!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: ['true', 'moderate', 'low', '?'] Prediction: riparian\n",
      "Query: ['true', 'moderate', 'medium', '?'] Prediction: riparian\n",
      "Query: ['true', 'moderate', 'highest', '?'] Prediction: conifer\n",
      "Query: ['true', 'moderate', 'high', '?'] Prediction: chaparral\n",
      "Query: ['true', 'steep', 'high', '?'] Prediction: chaparral\n",
      "Query: ['true', 'flat', 'high', '?'] Prediction: conifer\n"
     ]
    }
   ],
   "source": [
    "query1 = ['true','moderate','low','?']\n",
    "print(\"Query: \" + str(query1) + \" Prediction: \" + make_prediction(query1, tree, feature_levels))\n",
    "\n",
    "query2 = ['true','moderate','medium','?']\n",
    "print(\"Query: \" + str(query2) + \" Prediction: \" + make_prediction(query2, tree, feature_levels))\n",
    "\n",
    "query3 = ['true','moderate','highest','?']\n",
    "print(\"Query: \" + str(query3) + \" Prediction: \" + make_prediction(query3, tree, feature_levels))\n",
    "\n",
    "query4 = ['true','moderate','high','?']\n",
    "print(\"Query: \" + str(query4) + \" Prediction: \" + make_prediction(query4, tree, feature_levels))\n",
    "\n",
    "query5 = ['true','steep','high','?']\n",
    "print(\"Query: \" + str(query5) + \" Prediction: \" + make_prediction(query5, tree, feature_levels))\n",
    "\n",
    "query6 = ['true','flat','high','?']\n",
    "print(\"Query: \" + str(query6) + \" Prediction: \" + make_prediction(query6, tree, feature_levels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "d862ea1f4a663ab4dc121104b5479dcae532fa2a6e71ae1454e6742a55ab2bee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
